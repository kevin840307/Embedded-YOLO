{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\serialization.py:657: SourceChangeWarning: source code of class 'models.yolo.Model' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\serialization.py:657: SourceChangeWarning: source code of class 'models.common.DepthBottleneckCSP' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\serialization.py:657: SourceChangeWarning: source code of class 'models.common.Concat' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\serialization.py:657: SourceChangeWarning: source code of class 'models.yolo.Detect' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... Model Summary: 152 layers, 4.67189e+06 parameters, 3.92806e+06 gradients, 15.0 GFLOPS\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Sucessful: 180 / 182\n",
      "180\n",
      "['model.24.anchors', 'model.24.anchor_grid']\n",
      "Done! Good job!\n",
      "WARNING:tensorflow:From <ipython-input-1-96100ca9835f>:407: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./TF\\model\\saved_model.pb\n",
      "WARNING:tensorflow:From <ipython-input-1-96100ca9835f>:411: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 180 variables.\n",
      "INFO:tensorflow:Converted 180 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from models.experimental import *\n",
    "from utils.datasets import *\n",
    "import csv\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "\n",
    "model_path = './embedded_yolo.pt'\n",
    "param_path = './embedded_yolo.dict'\n",
    "save_dtype = tf.float32\n",
    "input_shape=[288, 480]\n",
    "strides = [8, 16, 32]\n",
    "class_num=4\n",
    "feat_size = [[input_shape[0] // s, input_shape[1] // s] for s in strides]\n",
    "\n",
    "def generate_dict(model_path, param_path):\n",
    "    model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model['model'].eval().fuse()\n",
    "    data_dict={}\n",
    "\n",
    "    #for param in model['model'].parameters():\n",
    "    #    print(param.shape)\n",
    "\n",
    "    for k, v in model['model'].state_dict().items():\n",
    "        vr = v.cpu().numpy()\n",
    "        data_dict[k]=vr\n",
    "        #print(k, ' ', vr.shape)\n",
    "\n",
    "    fid=open(param_path,'wb')   \n",
    "    pickle.dump(data_dict,fid)\n",
    "    fid.close()\n",
    "    \n",
    "def bn(input, name='bn'):\n",
    "    with tf.variable_scope(name):\n",
    "        gamma=tf.Variable(tf.random_normal(shape=[input.shape[-1].value]), name='weight',trainable=False)\n",
    "        beta = tf.Variable(tf.random_normal(shape=[input.shape[-1].value]), name='bias',trainable=False)\n",
    "        mean = tf.Variable(tf.random_normal(shape=[input.shape[-1].value]), name='running_mean',trainable=False)\n",
    "        var = tf.Variable(tf.random_normal(shape=[input.shape[-1].value]), name='running_var',trainable=False)\n",
    "\n",
    "        out=tf.nn.batch_normalization(input,mean,var,beta,gamma,variance_epsilon=0.001)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def conv(input,out_channels,ksize,stride,name='conv',add_bias=False):\n",
    "    filter = tf.Variable(tf.random_normal(shape=[ksize, ksize, input.shape[-1].value, out_channels], dtype=save_dtype), dtype=save_dtype,name=name+'/weight',trainable=False)\n",
    "    if ksize>1:\n",
    "        pad_h,pad_w=ksize//2,ksize//2\n",
    "        paddings = tf.constant([[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
    "        input = tf.pad(input, paddings, 'CONSTANT')\n",
    "    net = tf.nn.conv2d(input, filter, [1,stride, stride, 1], padding=\"VALID\")\n",
    "    \n",
    "    if add_bias:\n",
    "        bias = tf.Variable(tf.random_normal(shape=[out_channels], dtype=save_dtype),\n",
    "                             name=name + '/bias',trainable=False, dtype=save_dtype)\n",
    "        net=tf.nn.bias_add(net,bias)\n",
    "    return net\n",
    "\n",
    "\n",
    "def convBnLeakly(input,out_channels,ksize,stride,name):\n",
    "    with tf.variable_scope(name):\n",
    "        net=conv(input,out_channels,ksize,stride, add_bias=True)\n",
    "        #net=bn(net)\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "        return net\n",
    "    \n",
    "    \n",
    "def DepthWiseConv(input,ksize,stride,name='conv',add_bias=False):\n",
    "    out_channels = input.shape[-1].value\n",
    "    filter = tf.Variable(tf.random_normal(shape=[ksize, ksize, out_channels, 1], dtype=save_dtype), dtype=save_dtype,name=name+'/weight',trainable=False)\n",
    "\n",
    "    if ksize>1:\n",
    "        pad_h,pad_w=ksize//2,ksize//2\n",
    "        paddings = tf.constant([[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
    "        input = tf.pad(input, paddings, 'CONSTANT')\n",
    "    net = tf.nn.depthwise_conv2d(input, filter, [1,stride, stride, 1], padding=\"VALID\")\n",
    "    \n",
    "    if add_bias:\n",
    "        bias = tf.Variable(tf.random_normal(shape=[out_channels], dtype=save_dtype),\n",
    "                             name=name + '/bias',trainable=False, dtype=save_dtype)\n",
    "        net=tf.nn.bias_add(net,bias)\n",
    "    return net\n",
    "\n",
    "\n",
    "    \n",
    "def DepthWiseConvBnLeakly(input, out_channels,ksize,stride,name, e=1.5):\n",
    "    hidden_dim = int(input.shape[-1].value * e)\n",
    "    with tf.variable_scope(name):\n",
    "        net=conv(input,hidden_dim,1,1,name='0')\n",
    "        net=bn(net,name='1')\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "        \n",
    "        net=DepthWiseConv(net,ksize, stride,name='3')\n",
    "        net=bn(net,name='4')\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "        \n",
    "        net=conv(net,out_channels,1,1,name='6')\n",
    "        net=bn(net,name='7')\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "        \n",
    "        return net\n",
    "    \n",
    "def InvertedResidual(input,c1,c2,shortcut,e,name):\n",
    "    identity = shortcut and c1 == c2\n",
    "    with tf.variable_scope(name + '/conv'):\n",
    "        conv = DepthWiseConvBnLeakly(input, c2, 3, 1, 'conv')\n",
    "        \n",
    "        if identity:\n",
    "            return input + conv\n",
    "        else:\n",
    "            return conv\n",
    "    \n",
    "def DepthBottleneckCSP(input,c1,c2,n,shortcut,e,name):\n",
    "    c_=int(c2*e)\n",
    "    with tf.variable_scope(name):\n",
    "        net1=convBnLeakly(input,c_,1,1,'cv1')\n",
    "        for i in range(n):\n",
    "            net1=InvertedResidual(net1,c_,c_,shortcut,1.0,name='m/%d'%i)\n",
    "        net1=conv(net1,c_,1,1,name='cv3')\n",
    "\n",
    "        net2 = conv(input, c_, 1, 1, 'cv2')\n",
    "\n",
    "        net=tf.concat((net1,net2),-1)\n",
    "        net=bn(net)\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "\n",
    "        net=convBnLeakly(net,c2,1,1,'cv4')\n",
    "        return net\n",
    "    \n",
    "\n",
    "def focus(input,out_channels,ksize,name):\n",
    "    s1=input[:,::2,::2,:]\n",
    "    s2=input[:,1::2,::2,:]\n",
    "    s3 = input[:, ::2, 1::2, :]\n",
    "    s4 = input[:, 1::2, 1::2, :]\n",
    "        \n",
    "    net=tf.concat([s1,s2,s3,s4],axis=-1)            \n",
    "    net=convBnLeakly(net,out_channels,ksize,1,name+'/conv')\n",
    "    return net\n",
    "\n",
    "def bottleneck(input,c1,c2,shortcut,e,name):\n",
    "    with tf.variable_scope(name):\n",
    "        net=convBnLeakly(input,int(c2*e),1,1,'cv1')\n",
    "        net=convBnLeakly(net,c2,3,1,'cv2')\n",
    "\n",
    "        if (shortcut and c1==c2):\n",
    "            net+=input\n",
    "        return net\n",
    "\n",
    "def bottleneckCSP(input,c1,c2,n,shortcut,e,name):\n",
    "    c_=int(c2*e)\n",
    "    with tf.variable_scope(name):\n",
    "        net1=convBnLeakly(input,c_,1,1,'cv1')\n",
    "        for i in range(n):\n",
    "            net1=bottleneck(net1,c_,c_,shortcut,1.0,name='m/%d'%i)\n",
    "        net1=conv(net1,c_,1,1,name='cv3')\n",
    "\n",
    "        net2 = conv(input, c_, 1, 1, 'cv2')\n",
    "\n",
    "        net=tf.concat((net1,net2),-1)\n",
    "        net=bn(net)\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "\n",
    "        net=convBnLeakly(net,c2,1,1,'cv4')\n",
    "        return net\n",
    "    \n",
    "def spp(input,c1,c2,k1,k2,k3,name):\n",
    "    c_=c1//2\n",
    "    with tf.variable_scope(name):\n",
    "        net=convBnLeakly(input,c_,1,1,'cv1')\n",
    "\n",
    "        net1=tf.nn.max_pool(net,ksize=[1,k1,k1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "        net2=tf.nn.max_pool(net,ksize=[1,k2,k2,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "        net3 = tf.nn.max_pool(net, ksize=[1, k3, k3, 1], strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "        net=tf.concat((net,net1,net2,net3),-1)\n",
    "\n",
    "        net=convBnLeakly(net,c2,1,1,'cv2')\n",
    "        return net\n",
    "    \n",
    "def yolov5(input,class_num):\n",
    "    depth_multiple = 0.33\n",
    "    width_multiple = 0.5\n",
    "    \n",
    "    \n",
    "    w1 = int(round(64 * width_multiple))\n",
    "    w2 = int(round(128 * width_multiple))\n",
    "    w3 = int(round(256 * width_multiple))\n",
    "    w4 = int(round(512 * width_multiple))\n",
    "    w5 = int(round(1024 * width_multiple))\n",
    "\n",
    "    d1 = int(max(round(3 * depth_multiple), 1))\n",
    "    d2 = int(max(round(9 * depth_multiple), 1))\n",
    "\n",
    "    focus0=focus(input,w1,3,'model/0')\n",
    "    \n",
    "    conv1=convBnLeakly(focus0,w2,3,2,'model/1')\n",
    "    bottleneck_csp2=bottleneckCSP(conv1,w2,w2,d1,True,0.5,'model/2')\n",
    "    conv3 = convBnLeakly(bottleneck_csp2, w3, 3, 2, 'model/3')\n",
    "    bottleneck_csp4 = bottleneckCSP(conv3, w3, w3, d2, True, 0.5, 'model/4')\n",
    "    conv5 = convBnLeakly(bottleneck_csp4, w4, 3, 2, 'model/5')\n",
    "    bottleneck_csp6 = bottleneckCSP(conv5, w4, w4, d2, True, 0.5, 'model/6')\n",
    "    conv7 = DepthWiseConvBnLeakly(bottleneck_csp6, w4, 3, 2, 'model/7/conv')\n",
    "    spp8=spp(conv7,w4,w4,5,9,13,'model/8')\n",
    "\n",
    "    bottleneck_csp9 = DepthBottleneckCSP(spp8, w4, w4, d1, False, 0.5, 'model/9')\n",
    "    conv10 = convBnLeakly(bottleneck_csp9, w4, 1, 1, 'model/10')\n",
    "\n",
    "    shape=[conv10.shape[1].value*2,conv10.shape[2].value*2]\n",
    "    \n",
    "    deconv11=tf.image.resize_images(conv10,shape,method=1)\n",
    "\n",
    "    cat12=tf.concat((deconv11,bottleneck_csp6),-1)\n",
    "    bottleneck_csp13=bottleneckCSP(cat12, w5, w4, d1, False, 0.5, 'model/13')\n",
    "    conv14 = convBnLeakly(bottleneck_csp13, w3, 1, 1, 'model/14')\n",
    "\n",
    "    shape = [conv14.shape[1].value * 2, conv14.shape[2].value * 2]\n",
    "    deconv15 = tf.image.resize_images(conv14, shape,method=1)\n",
    "    \n",
    "    cat16 = tf.concat((deconv15, bottleneck_csp4), -1)\n",
    "    bottleneck_csp17 = bottleneckCSP(cat16, w4, w3, d1, False, 0.5, 'model/17')\n",
    "    conv18 = convBnLeakly(bottleneck_csp17, w3, 3, 2, 'model/18')\n",
    "\n",
    "    cat19 = tf.concat((conv18, conv14), -1)\n",
    "    bottleneck_csp20 = bottleneckCSP(cat19, w4, w4, d1, False, 0.5, 'model/20')\n",
    "    conv21 = convBnLeakly(bottleneck_csp20, w4, 3, 2, 'model/21')\n",
    "\n",
    "    cat22= tf.concat((conv21, conv10), -1)\n",
    "    bottleneck_csp23 = bottleneckCSP(cat22, w5, w5, d1, False, 0.5, 'model/23')\n",
    "\n",
    "    conv24m0=conv(bottleneck_csp17,3*(class_num+5),1,1,'model/24/m/0',add_bias=True)\n",
    "    conv24m1 = conv(bottleneck_csp20, 3 * (class_num + 5), 1, 1, 'model/24/m/1',add_bias=True)\n",
    "    conv24m2 = conv(bottleneck_csp23, 3 * (class_num + 5), 1, 1, 'model/24/m/2',add_bias=True)\n",
    "    return conv24m0,conv24m1,conv24m2\n",
    "\n",
    "    \n",
    "def post_process(inputs,grids,strides,anchor_grid,class_num, iou_th=0.5, conf_th=0.03, is_multiple=False):\n",
    "\n",
    "    total=[]\n",
    "    for i,logits in enumerate(inputs):\n",
    "        logits = tf.cast(logits, tf.float32)\n",
    "        nb=logits.shape[0]#.value\n",
    "        ny = logits.shape[1]#.value\n",
    "        nx = logits.shape[2]#.value\n",
    "        nc = logits.shape[3]#.value\n",
    "\n",
    "        logits=tf.reshape(logits,[nb,ny,nx,3,nc//3])\n",
    "        logits=tf.sigmoid(logits)\n",
    "\n",
    "        logits_xy=(logits[...,:2]*2.-0.5+grids[i])*strides[i]\n",
    "        logits_wh = ((logits[...,2:4] * 2)**2)*anchor_grid[i]\n",
    "\n",
    "        logits_new=tf.concat((logits_xy,logits_wh,logits[...,4:]),axis=-1)\n",
    "\n",
    "        total.append(tf.reshape(logits_new,[-1,nc//3]))\n",
    "    total=tf.concat(total,axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mask = total[:, 4] > conf_th\n",
    "    total = tf.boolean_mask(total, mask)\n",
    "\n",
    "    \n",
    "    x,y,w,h,conf,prob=tf.split(total,[1,1,1,1,1,class_num],axis=-1)\n",
    "    x1=x-w/2.\n",
    "    y1=y-h/2.\n",
    "    x2=x+w/2.\n",
    "    y2=y+h/2.\n",
    "    conf_prob=conf*prob\n",
    "    \n",
    "    if is_multiple:\n",
    "        scores=tf.reduce_max(conf_prob,axis=-1)\n",
    "        scores = tf.cast(scores,tf.float32)\n",
    "        labels=tf.cast(tf.argmax(conf_prob,axis=-1),tf.float32)\n",
    "\n",
    "        boxes=tf.concat([x1,y1,x2,y2],axis=1)\n",
    "        boxes=tf.cast(boxes,tf.float32)\n",
    "        \n",
    "        indices=tf.image.non_max_suppression(boxes,scores,max_output_size=1000,iou_threshold=iou_th,score_threshold=conf_th)\n",
    "    else:\n",
    "        scores = tf.cast(conf_prob, tf.float32)\n",
    "        scores = tf.reshape(scores, [-1])\n",
    "        \n",
    "        labels = tf.constant([0, 1, 2, 3], dtype=tf.float32)\n",
    "        labels = tf.tile(labels, [tf.shape(scores)[0] // 4])\n",
    "\n",
    "        boxes =tf.concat([x1,y1,x2,y2],axis=1)\n",
    "        boxes = tf.cast(boxes, tf.float32)\n",
    "        boxes = tf.tile(boxes, [1, 4])\n",
    "        boxes = tf.reshape(boxes, [-1, 4])\n",
    "        \n",
    "        scores_mask = scores > conf_th\n",
    "        labels = tf.boolean_mask(labels, scores_mask)\n",
    "        boxes = tf.boolean_mask(boxes, scores_mask)\n",
    "        scores = tf.boolean_mask(scores, scores_mask)\n",
    "        indices=tf.image.non_max_suppression(boxes + tf.reshape(labels, [-1, 1]) * 4096,scores,max_output_size=1000,iou_threshold=iou_th,score_threshold=conf_th)\n",
    "    \n",
    "    boxes=tf.gather(boxes,indices)\n",
    "    scores=tf.reshape(tf.gather(scores,indices),[-1,1])\n",
    "    labels=tf.reshape(tf.gather(labels,indices),[-1,1])\n",
    "\n",
    "    output=tf.concat([boxes,scores,labels],axis=-1)\n",
    "    return output\n",
    "\n",
    "def read_dict(param_path, feat_size):\n",
    "    weights=open(param_path,'rb')\n",
    "    params_dict = pickle.load(weights)\n",
    "    grids = []\n",
    "    for size in feat_size:\n",
    "        ny, nx = size\n",
    "        import torch\n",
    "        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
    "        grid= torch.stack((xv, yv), 2).view((1, ny, nx,1, 2)).float().numpy()\n",
    "\n",
    "        grid = tf.convert_to_tensor(grid, tf.float32)\n",
    "        grids.append(grid)\n",
    "    anchors = params_dict['model.24.anchors']\n",
    "    anchor_gird = params_dict['model.24.anchor_grid']\n",
    "    anchor_gird = np.transpose(anchor_gird, (0, 1, 3, 4, 2, 5))\n",
    "    anchor_gird = anchor_gird.astype(np.float32)\n",
    "    return params_dict, anchor_gird, grids\n",
    "    \n",
    "def get_tf_assign_op(params_dict):\n",
    "    vars = tf.global_variables()\n",
    "    list_layer = []\n",
    "    total_layer = 0\n",
    "    for params in params_dict.keys():\n",
    "        if params.find(\"num_batches_tracked\") == -1:\n",
    "            total_layer += 1\n",
    "            list_layer.append(params)\n",
    "\n",
    "\n",
    "    sucessful = 0\n",
    "    assign_ops = []\n",
    "    dephe_convs = ['model.7.conv.3.weight',\n",
    "                   'model.9.m.0.conv.conv.3.weight']\n",
    "    for var in vars:\n",
    "        name = var.name[:-2].replace(\"/\",'.')\n",
    "        try:\n",
    "            params=params_dict[name]\n",
    "            if len(params.shape) == 4:\n",
    "                if name in dephe_convs:\n",
    "                    params=np.transpose(params,(2,3,0,1))\n",
    "                else:   \n",
    "                    params=np.transpose(params,(2,3,1,0))\n",
    "\n",
    "            # print(p.shape, var)\n",
    "            assign_ops.append(tf.assign(var, params))\n",
    "            sucessful += 1\n",
    "            list_layer.remove(name)\n",
    "        except:\n",
    "            print(\"load wedits error:\", name, ' ', var.shape)\n",
    "    print(\"Sucessful:\", sucessful, '/', total_layer)\n",
    "    print(len(vars))\n",
    "    print(list_layer)\n",
    "    return assign_ops\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', type=str, default='./embedded_yolo.pt')\n",
    "    parser.add_argument('--param_path', type=str, default='./embedded_yolo.dict')\n",
    "    parser.add_argument('--save_path', type=str, default='./TF')\n",
    "    parser.add_argument('--type', type=int, default=1)\n",
    "    \n",
    "    #opt = parser.parse_args()\n",
    "    opt, unparsed = parser.parse_known_args()\n",
    "    if opt.type == 1:\n",
    "        save_dtype = tf.float16\n",
    "        \n",
    "    model_path = opt.model_path\n",
    "    param_path = opt.param_path\n",
    "\n",
    "    \n",
    "    generate_dict(model_path, param_path)\n",
    "    params_dict, anchor_gird, grids = read_dict(param_path, feat_size)\n",
    "\n",
    "    input=tf.placeholder(save_dtype, shape=[1, input_shape[0],input_shape[1],3],name='input')\n",
    "    logits=yolov5(input,class_num)\n",
    "    logit1=tf.identity(logits[0],'out_logit1')\n",
    "    logit2=tf.identity(logits[1],'out_logit2')\n",
    "    logit3=tf.identity(logits[2],'out_logit3')\n",
    "    output=post_process(logits,grids,strides,anchor_gird,class_num)\n",
    "    output=tf.identity(output,'output')\n",
    "\n",
    "    assign_ops = get_tf_assign_op(params_dict)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(assign_ops)\n",
    "        print(\"Done! Good job!\")\n",
    "        \n",
    "        import shutil\n",
    "        if os.path.isdir(os.path.join(opt.save_path, 'model')):\n",
    "            shutil.rmtree(os.path.join(opt.save_path, 'model'))\n",
    "            \n",
    "        if os.path.isdir(os.path.join(opt.save_path, 'lite_model')):\n",
    "            shutil.rmtree(os.path.join(opt.save_path, 'lite_model'))\n",
    "\n",
    "            \n",
    "        tf.saved_model.simple_save(sess, os.path.join(opt.save_path, 'model'), \n",
    "                                       inputs={\"inputs\": input},\n",
    "                                       outputs={\"output\": output})\n",
    "            \n",
    "        tf.saved_model.simple_save(sess, os.path.join(opt.save_path, 'lite_model'), \n",
    "                                       inputs={\"inputs\": input},\n",
    "                                       outputs = {\"out_logit1\": logit1, \"out_logit2\": logit2, \"out_logit3\": logit3})\n",
    "        \n",
    "        converted_graph_def = tf.graph_util.convert_variables_to_constants(sess,\n",
    "                                  input_graph_def=sess.graph.as_graph_def(),\n",
    "                                  output_node_names=[\"output\"])\n",
    "        \n",
    "        with tf.gfile.GFile(os.path.join(opt.save_path, 'Frozen_model.pb'), \"wb\") as f:\n",
    "            f.write(converted_graph_def.SerializeToString())\n",
    "\n",
    "    if save_dtype == tf.float32:\n",
    "        converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(os.path.join(opt.save_path, 'lite_model'))\n",
    "        tflite_model = converter.convert()\n",
    "        with open(os.path.join(opt.save_path, 'model.tflite'), 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
