{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "save_dtype = tf.float32\n",
    "\n",
    "def bn(input, name='bn'):\n",
    "    with tf.variable_scope(name):\n",
    "        gamma=tf.Variable(tf.random_normal(shape=[input.shape[-1].value]), name='weight',trainable=False)\n",
    "        beta = tf.Variable(tf.random_normal(shape=[input.shape[-1].value]), name='bias',trainable=False)\n",
    "        mean = tf.Variable(tf.random_normal(shape=[input.shape[-1].value]), name='running_mean',trainable=False)\n",
    "        var = tf.Variable(tf.random_normal(shape=[input.shape[-1].value]), name='running_var',trainable=False)\n",
    "\n",
    "        out=tf.nn.batch_normalization(input,mean,var,beta,gamma,variance_epsilon=0.001)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def conv(input,out_channels,ksize,stride,name='conv',add_bias=False):\n",
    "    filter = tf.Variable(tf.random_normal(shape=[ksize, ksize, input.shape[-1].value, out_channels], dtype=save_dtype), dtype=save_dtype,name=name+'/weight',trainable=False)\n",
    "    if ksize>1:\n",
    "        pad_h,pad_w=ksize//2,ksize//2\n",
    "        paddings = tf.constant([[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
    "        input = tf.pad(input, paddings, 'CONSTANT')\n",
    "    net = tf.nn.conv2d(input, filter, [1,stride, stride, 1], padding=\"VALID\")\n",
    "    \n",
    "    if add_bias:\n",
    "        bias = tf.Variable(tf.random_normal(shape=[out_channels], dtype=save_dtype),\n",
    "                             name=name + '/bias',trainable=False, dtype=save_dtype)\n",
    "        net=tf.nn.bias_add(net,bias)\n",
    "    return net\n",
    "\n",
    "\n",
    "def convBnLeakly(input,out_channels,ksize,stride,name):\n",
    "    with tf.variable_scope(name):\n",
    "        net=conv(input,out_channels,ksize,stride, add_bias=True)\n",
    "        #net=bn(net)\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "        return net\n",
    "    \n",
    "    \n",
    "def DepthWiseConv(input,ksize,stride,name='conv',add_bias=False):\n",
    "    out_channels = input.shape[-1].value\n",
    "    filter = tf.Variable(tf.random_normal(shape=[ksize, ksize, out_channels, 1], dtype=save_dtype), dtype=save_dtype,name=name+'/weight',trainable=False)\n",
    "\n",
    "    if ksize>1:\n",
    "        pad_h,pad_w=ksize//2,ksize//2\n",
    "        paddings = tf.constant([[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
    "        input = tf.pad(input, paddings, 'CONSTANT')\n",
    "    net = tf.nn.depthwise_conv2d(input, filter, [1,stride, stride, 1], padding=\"VALID\")\n",
    "    \n",
    "    if add_bias:\n",
    "        bias = tf.Variable(tf.random_normal(shape=[out_channels], dtype=save_dtype),\n",
    "                             name=name + '/bias',trainable=False, dtype=save_dtype)\n",
    "        net=tf.nn.bias_add(net,bias)\n",
    "    return net\n",
    "\n",
    "\n",
    "    \n",
    "def DepthWiseConvBnLeakly(input, out_channels,ksize,stride,name, e=1.5):\n",
    "    hidden_dim = int(input.shape[-1].value * e)\n",
    "    with tf.variable_scope(name):\n",
    "        net=conv(input,hidden_dim,1,1,name='0')\n",
    "        net=bn(net,name='1')\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "        \n",
    "        net=DepthWiseConv(net,ksize, stride,name='3')\n",
    "        net=bn(net,name='4')\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "        \n",
    "        net=conv(net,out_channels,1,1,name='6')\n",
    "        net=bn(net,name='7')\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "        \n",
    "        return net\n",
    "    \n",
    "def InvertedResidual(input,c1,c2,shortcut,e,name):\n",
    "    identity = shortcut and c1 == c2\n",
    "    with tf.variable_scope(name + '/conv'):\n",
    "        conv = DepthWiseConvBnLeakly(input, c2, 3, 1, 'conv')\n",
    "        \n",
    "        if identity:\n",
    "            return input + conv\n",
    "        else:\n",
    "            return conv\n",
    "    \n",
    "def DepthBottleneckCSP(input,c1,c2,n,shortcut,e,name):\n",
    "    c_=int(c2*e)\n",
    "    with tf.variable_scope(name):\n",
    "        net1=convBnLeakly(input,c_,1,1,'cv1')\n",
    "        for i in range(n):\n",
    "            net1=InvertedResidual(net1,c_,c_,shortcut,1.0,name='m/%d'%i)\n",
    "        net1=conv(net1,c_,1,1,name='cv3')\n",
    "\n",
    "        net2 = conv(input, c_, 1, 1, 'cv2')\n",
    "\n",
    "        net=tf.concat((net1,net2),-1)\n",
    "        net=bn(net)\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "\n",
    "        net=convBnLeakly(net,c2,1,1,'cv4')\n",
    "        return net\n",
    "    \n",
    "\n",
    "def focus(input,out_channels,ksize,name):\n",
    "    s1=input[:,::2,::2,:]\n",
    "    s2=input[:,1::2,::2,:]\n",
    "    s3 = input[:, ::2, 1::2, :]\n",
    "    s4 = input[:, 1::2, 1::2, :]\n",
    "        \n",
    "    net=tf.concat([s1,s2,s3,s4],axis=-1)            \n",
    "    net=convBnLeakly(net,out_channels,ksize,1,name+'/conv')\n",
    "    return net\n",
    "\n",
    "def bottleneck(input,c1,c2,shortcut,e,name):\n",
    "    with tf.variable_scope(name):\n",
    "        net=convBnLeakly(input,int(c2*e),1,1,'cv1')\n",
    "        net=convBnLeakly(net,c2,3,1,'cv2')\n",
    "\n",
    "        if (shortcut and c1==c2):\n",
    "            net+=input\n",
    "        return net\n",
    "\n",
    "def bottleneckCSP(input,c1,c2,n,shortcut,e,name):\n",
    "    c_=int(c2*e)\n",
    "    with tf.variable_scope(name):\n",
    "        net1=convBnLeakly(input,c_,1,1,'cv1')\n",
    "        for i in range(n):\n",
    "            net1=bottleneck(net1,c_,c_,shortcut,1.0,name='m/%d'%i)\n",
    "        net1=conv(net1,c_,1,1,name='cv3')\n",
    "\n",
    "        net2 = conv(input, c_, 1, 1, 'cv2')\n",
    "\n",
    "        net=tf.concat((net1,net2),-1)\n",
    "        net=bn(net)\n",
    "        net=tf.nn.leaky_relu(net,alpha=0.1)\n",
    "\n",
    "        net=convBnLeakly(net,c2,1,1,'cv4')\n",
    "        return net\n",
    "    \n",
    "def spp(input,c1,c2,k1,k2,k3,name):\n",
    "    c_=c1//2\n",
    "    with tf.variable_scope(name):\n",
    "        net=convBnLeakly(input,c_,1,1,'cv1')\n",
    "\n",
    "        net1=tf.nn.max_pool(net,ksize=[1,k1,k1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "        net2=tf.nn.max_pool(net,ksize=[1,k2,k2,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "        net3 = tf.nn.max_pool(net, ksize=[1, k3, k3, 1], strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "        net=tf.concat((net,net1,net2,net3),-1)\n",
    "\n",
    "        net=convBnLeakly(net,c2,1,1,'cv2')\n",
    "        return net\n",
    "    \n",
    "def yolov5(input,class_num,model_name):\n",
    "    depth_multiple = 0.33\n",
    "    width_multiple = 0.5\n",
    "    \n",
    "    \n",
    "    w1 = int(round(64 * width_multiple))\n",
    "    w2 = int(round(128 * width_multiple))\n",
    "    w3 = int(round(256 * width_multiple))\n",
    "    w4 = int(round(512 * width_multiple))\n",
    "    w5 = int(round(1024 * width_multiple))\n",
    "\n",
    "    d1 = int(max(round(3 * depth_multiple), 1))\n",
    "    d2 = int(max(round(9 * depth_multiple), 1))\n",
    "\n",
    "    focus0=focus(input,w1,3,'model/0')\n",
    "    \n",
    "    conv1=convBnLeakly(focus0,w2,3,2,'model/1')\n",
    "    bottleneck_csp2=bottleneckCSP(conv1,w2,w2,d1,True,0.5,'model/2')\n",
    "    conv3 = convBnLeakly(bottleneck_csp2, w3, 3, 2, 'model/3')\n",
    "    bottleneck_csp4 = bottleneckCSP(conv3, w3, w3, d2, True, 0.5, 'model/4')\n",
    "    conv5 = convBnLeakly(bottleneck_csp4, w4, 3, 2, 'model/5')\n",
    "    bottleneck_csp6 = bottleneckCSP(conv5, w4, w4, d2, True, 0.5, 'model/6')\n",
    "    conv7 = DepthWiseConvBnLeakly(bottleneck_csp6, w4, 3, 2, 'model/7/conv')\n",
    "    spp8=spp(conv7,w4,w4,5,9,13,'model/8')\n",
    "\n",
    "    bottleneck_csp9 = DepthBottleneckCSP(spp8, w4, w4, d1, False, 0.5, 'model/9')\n",
    "    conv10 = convBnLeakly(bottleneck_csp9, w4, 1, 1, 'model/10')\n",
    "\n",
    "    shape=[conv10.shape[1].value*2,conv10.shape[2].value*2]\n",
    "    \n",
    "    deconv11=tf.image.resize_images(conv10,shape,method=1)\n",
    "\n",
    "    cat12=tf.concat((deconv11,bottleneck_csp6),-1)\n",
    "    bottleneck_csp13=bottleneckCSP(cat12, w5, w4, d1, False, 0.5, 'model/13')\n",
    "    conv14 = convBnLeakly(bottleneck_csp13, w3, 1, 1, 'model/14')\n",
    "\n",
    "    shape = [conv14.shape[1].value * 2, conv14.shape[2].value * 2]\n",
    "    deconv15 = tf.image.resize_images(conv14, shape,method=1)\n",
    "    \n",
    "    cat16 = tf.concat((deconv15, bottleneck_csp4), -1)\n",
    "    bottleneck_csp17 = bottleneckCSP(cat16, w4, w3, d1, False, 0.5, 'model/17')\n",
    "    conv18 = convBnLeakly(bottleneck_csp17, w3, 3, 2, 'model/18')\n",
    "\n",
    "    cat19 = tf.concat((conv18, conv14), -1)\n",
    "    bottleneck_csp20 = bottleneckCSP(cat19, w4, w4, d1, False, 0.5, 'model/20')\n",
    "    conv21 = convBnLeakly(bottleneck_csp20, w4, 3, 2, 'model/21')\n",
    "\n",
    "    cat22= tf.concat((conv21, conv10), -1)\n",
    "    bottleneck_csp23 = bottleneckCSP(cat22, w5, w5, d1, False, 0.5, 'model/23')\n",
    "\n",
    "    conv24m0=conv(bottleneck_csp17,3*(class_num+5),1,1,'model/24/m/0',add_bias=True)\n",
    "    conv24m1 = conv(bottleneck_csp20, 3 * (class_num + 5), 1, 1, 'model/24/m/1',add_bias=True)\n",
    "    conv24m2 = conv(bottleneck_csp23, 3 * (class_num + 5), 1, 1, 'model/24/m/2',add_bias=True)\n",
    "    return conv24m0,conv24m1,conv24m2\n",
    "\n",
    "    \n",
    "def post_process(inputs,grids,strides,anchor_grid,class_num, iou_th=0.5, conf_th=0.03):\n",
    "\n",
    "    total=[]\n",
    "    for i,logits in enumerate(inputs):\n",
    "        logits = tf.cast(logits, tf.float32)\n",
    "        nb=logits.shape[0]#.value\n",
    "        ny = logits.shape[1]#.value\n",
    "        nx = logits.shape[2]#.value\n",
    "        nc = logits.shape[3]#.value\n",
    "\n",
    "        logits=tf.reshape(logits,[nb,ny,nx,3,nc//3])\n",
    "        logits=tf.sigmoid(logits)\n",
    "\n",
    "        logits_xy=(logits[...,:2]*2.-0.5+grids[i])*strides[i]\n",
    "        logits_wh = ((logits[...,2:4] * 2)**2)*anchor_grid[i]\n",
    "\n",
    "        logits_new=tf.concat((logits_xy,logits_wh,logits[...,4:]),axis=-1)\n",
    "\n",
    "        total.append(tf.reshape(logits_new,[-1,nc//3]))\n",
    "    total=tf.concat(total,axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mask = total[:, 4] > conf_th\n",
    "    total = tf.boolean_mask(total, mask)\n",
    "\n",
    "    \n",
    "    x,y,w,h,conf,prob=tf.split(total,[1,1,1,1,1,class_num],axis=-1)\n",
    "    x1=x-w/2.\n",
    "    y1=y-h/2.\n",
    "    x2=x+w/2.\n",
    "    y2=y+h/2.\n",
    "    conf_prob=conf*prob\n",
    "    \n",
    "    if False:\n",
    "        scores=tf.reduce_max(conf_prob,axis=-1)\n",
    "        scores = tf.cast(scores,tf.float32)\n",
    "        labels=tf.cast(tf.argmax(conf_prob,axis=-1),tf.float32)\n",
    "\n",
    "        boxes=tf.concat([x1,y1,x2,y2],axis=1)\n",
    "        boxes=tf.cast(boxes,tf.float32)\n",
    "        \n",
    "        indices=tf.image.non_max_suppression(boxes,scores,max_output_size=1000,iou_threshold=iou_th,score_threshold=conf_th)\n",
    "    else:\n",
    "        scores = tf.cast(conf_prob, tf.float32)\n",
    "        scores = tf.reshape(scores, [-1])\n",
    "        \n",
    "        labels = tf.constant([0, 1, 2, 3], dtype=tf.float32)\n",
    "        labels = tf.tile(labels, [tf.shape(scores)[0] // 4])\n",
    "\n",
    "        boxes =tf.concat([x1,y1,x2,y2],axis=1)\n",
    "        boxes = tf.cast(boxes, tf.float32)\n",
    "        boxes = tf.tile(boxes, [1, 4])\n",
    "        boxes = tf.reshape(boxes, [-1, 4])\n",
    "        \n",
    "        scores_mask = scores > conf_th\n",
    "        labels = tf.boolean_mask(labels, scores_mask)\n",
    "        boxes = tf.boolean_mask(boxes, scores_mask)\n",
    "        scores = tf.boolean_mask(scores, scores_mask)\n",
    "        indices=tf.image.non_max_suppression(boxes + tf.reshape(labels, [-1, 1]) * 4096,scores,max_output_size=1000,iou_threshold=iou_th,score_threshold=conf_th)\n",
    "    \n",
    "    boxes=tf.gather(boxes,indices)\n",
    "    scores=tf.reshape(tf.gather(scores,indices),[-1,1])\n",
    "    labels=tf.reshape(tf.gather(labels,indices),[-1,1])\n",
    "\n",
    "    output=tf.concat([boxes,scores,labels],axis=-1)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#384, 640\n",
    "#320, 512\n",
    "param_path = '../embedded_yolo.dict'\n",
    "input_shape=[288, 480]\n",
    "strides = [8, 16, 32]\n",
    "\n",
    "class_num=4\n",
    "model_name='s'\n",
    "feat_size = [[input_shape[0] // s, input_shape[1] // s] for s in strides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import torch\n",
    "import os\n",
    "fid=open(param_path,'rb')\n",
    "params_dict=pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = []\n",
    "for size in feat_size:\n",
    "    ny, nx = size\n",
    "    import torch\n",
    "    yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
    "    grid= torch.stack((xv, yv), 2).view((1, ny, nx,1, 2)).float().numpy()\n",
    "\n",
    "    grid = tf.convert_to_tensor(grid, tf.float32)\n",
    "    grids.append(grid)\n",
    "    \n",
    "    \n",
    "anchors = params_dict['model.24.anchors']\n",
    "anchor_gird = params_dict['model.24.anchor_grid']\n",
    "anchor_gird = np.transpose(anchor_gird, (0, 1, 3, 4, 2, 5))\n",
    "anchor_gird = anchor_gird.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "input=tf.placeholder(save_dtype, shape=[1, input_shape[0],input_shape[1],3],name='input')\n",
    "logits=yolov5(input,class_num,model_name)\n",
    "logit1=tf.identity(logits[0],'out_logit1')\n",
    "logit2=tf.identity(logits[1],'out_logit2')\n",
    "logit3=tf.identity(logits[2],'out_logit3')\n",
    "output=post_process(logits,grids,strides,anchor_gird,class_num)\n",
    "output=tf.identity(output,'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessful: 180 / 182\n",
      "180\n",
      "['model.24.anchors', 'model.24.anchor_grid']\n"
     ]
    }
   ],
   "source": [
    "vars = tf.global_variables()\n",
    "list_layer = []\n",
    "total_layer = 0\n",
    "for params in params_dict.keys():\n",
    "    if params.find(\"num_batches_tracked\") == -1:\n",
    "        total_layer += 1\n",
    "        list_layer.append(params)\n",
    "        \n",
    "        \n",
    "sucessful = 0\n",
    "dephe_convs = ['model.7.conv.3.weight',\n",
    "               'model.9.m.0.conv.conv.3.weight']\n",
    "assign_ops = []\n",
    "for var in vars:\n",
    "    name = var.name[:-2].replace(\"/\",'.')\n",
    "    try:\n",
    "        p=params_dict[name]\n",
    "        if len(p.shape)==4:\n",
    "            if name in dephe_convs:\n",
    "                p=np.transpose(p,(2,3,0,1))\n",
    "            else:   \n",
    "                p=np.transpose(p,(2,3,1,0))\n",
    "\n",
    "        # print(p.shape, var)\n",
    "        assign_ops.append(tf.assign(var,p))\n",
    "        sucessful += 1\n",
    "        list_layer.remove(name)\n",
    "    except:\n",
    "        print(\"load wedits error:\", name, ' ', var.shape)\n",
    "print(\"Sucessful:\", sucessful, '/', total_layer)\n",
    "print(len(vars))\n",
    "print(list_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Good job!\n",
      "WARNING:tensorflow:From <ipython-input-7-29530fbe6ecd>:12: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./model\\saved_model.pb\n",
      "WARNING:tensorflow:From <ipython-input-7-29530fbe6ecd>:19: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 180 variables.\n",
      "INFO:tensorflow:Converted 180 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(assign_ops)\n",
    "    print(\"Done! Good job!\")\n",
    "    \n",
    "    import shutil\n",
    "    if os.path.isdir('./model'):\n",
    "        shutil.rmtree('./model')\n",
    "    \n",
    "    tf.saved_model.simple_save(sess, './model', \n",
    "                               inputs={\"inputs\": input},\n",
    "                               outputs={\"out_logit1\": logit1, \"out_logit2\": logit2, \"out_logit3\": logit3})\n",
    "    #{\"out_logit1\": logit1, \"out_logit2\": logit2, \"out_logit3\": logit3}\n",
    "    #{\"output\": output}\n",
    "    \n",
    "    \n",
    "    converted_graph_def = tf.graph_util.convert_variables_to_constants(sess,\n",
    "                              input_graph_def=sess.graph.as_graph_def(),\n",
    "                              output_node_names=[\"output\"])\n",
    "    with tf.gfile.GFile('./Frozen_model.pb', \"wb\") as f:\n",
    "        f.write(converted_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\lite\\python\\convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model\\variables\\variables\n",
      "INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\n",
      "INFO:tensorflow:input tensors info: \n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: inputs\n",
      "INFO:tensorflow: tensor name: input:0, shape: (1, 288, 480, 3), type: DT_FLOAT\n",
      "INFO:tensorflow:output tensors info: \n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: out_logit1\n",
      "INFO:tensorflow: tensor name: out_logit1:0, shape: (1, 36, 60, 27), type: DT_FLOAT\n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: out_logit2\n",
      "INFO:tensorflow: tensor name: out_logit2:0, shape: (1, 18, 30, 27), type: DT_FLOAT\n",
      "INFO:tensorflow:Tensor's key in saved_model's tensor_map: out_logit3\n",
      "INFO:tensorflow: tensor name: out_logit3:0, shape: (1, 9, 15, 27), type: DT_FLOAT\n",
      "INFO:tensorflow:Restoring parameters from ./model\\variables\\variables\n",
      "INFO:tensorflow:Froze 180 variables.\n",
      "INFO:tensorflow:Converted 180 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model('./model')\n",
    "tflite_model = converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
